{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T18:27:18.373965Z",
     "start_time": "2020-05-14T18:27:18.359951Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "spark = SparkSession.builder.appName(\"spark_stream\").getOrCreate()\n",
    "sqlCon = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n",
    "\n",
    "from pyspark.streaming import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set IP and Port\n",
    "\n",
    "Enter the IP and port at which `stream.py` is sending edit stream to.\n",
    "Run the `stream.py` file using \n",
    "```\n",
    "python stream.py \"en.wikipedia.org\" 5050 \"20200515\"\n",
    "\n",
    "```\n",
    "\n",
    "**en.wikipedia.org**:   English wikipedia edit stream is used here for example. Other languages can be used like for `German` use `de.wikipedia.org`\n",
    "\n",
    "**5050**:   is the port at which edit stream will be written\n",
    "\n",
    "**20200515**:   date from where edits will be reported till the latest. Format of date is `YYYYMMDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T18:56:39.884288Z",
     "start_time": "2020-05-14T18:56:39.871572Z"
    }
   },
   "outputs": [],
   "source": [
    "# time after which new rdd is generated\n",
    "batchInterval = 3\n",
    "\n",
    "ServerHost  = \"127.0.0.1\"\n",
    "ServerPort  = 5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T18:56:46.844623Z",
     "start_time": "2020-05-14T18:56:46.702065Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start streaming context with current spark context\n",
    "ssc = StreamingContext(spark.sparkContext, batchDuration=batchInterval)\n",
    "\n",
    "# instantiate new stream with given streaming context\n",
    "englishStream = ssc.socketTextStream(ServerHost, ServerPort)\n",
    "\n",
    "# time in milliseconds for which table will be saved in the memory\n",
    "ssc.remember(2400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Stream Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T18:56:53.872827Z",
     "start_time": "2020-05-14T18:56:53.854643Z"
    }
   },
   "outputs": [],
   "source": [
    "# read new rdd after batchinterval, convert data from json to DataFrame and register temp table\n",
    "\n",
    "def convert_data(time, rdd):\n",
    "    try:\n",
    "        sqlCon.read.json(rdd).registerTempTable(\"English_Edits\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T18:57:38.663244Z",
     "start_time": "2020-05-14T18:57:38.536465Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# register window operation on stream data. Make one partition of whole data and than make temp table\n",
    "\n",
    "# windowDuration defines the number of batches retained\n",
    "# slideDuration defines the step size for windowing\n",
    "\n",
    "englishStream.window(windowDuration=60, slideDuration=3).repartition(1).foreachRDD(convert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T19:23:12.085067Z",
     "start_time": "2020-05-14T19:23:11.981484Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start streaming context\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T19:22:49.925897Z",
     "start_time": "2020-05-14T19:22:47.739851Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a peek on the data retrieved from Wikipedia \n",
    "\n",
    "df = spark.sql(\"SELECT * FROM English_Edits\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:29:11.760498Z",
     "start_time": "2020-05-14T17:28:19.831842Z"
    }
   },
   "outputs": [],
   "source": [
    "# graphical summary of number of edits happened in last windowDuration\n",
    "\n",
    "df = spark.sql(\"SELECT timestamp, count(*) as count FROM English_Edits GROUP BY timestamp ORDER BY timestamp\")\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x=df_pd[\"timestamp\"], height=df_pd[\"count\"], width=0.25, color=\"Orange\")\n",
    "loc = plticker.MultipleLocator(base=25.0) # this locator puts ticks at regular intervals\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "plt.xticks(rotation=45)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T19:22:34.360283Z",
     "start_time": "2020-05-14T19:22:29.374201Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stop streaming context\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T18:10:43.691495Z",
     "start_time": "2020-05-14T18:10:43.665082Z"
    }
   },
   "outputs": [],
   "source": [
    "# stop spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}